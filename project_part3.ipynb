{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal and Significance\n",
    "\n",
    "### Goal\n",
    "\n",
    "The objective of Part 3 of this project is to **predict** the product rating by leveraging **product features** (e.g., product name, loves count, average rating, etc.). We also plan to analyze which features contribute most to product ratings, providing meaningful insights into customer preferences and product considerations.\n",
    "\n",
    "### Significance\n",
    "This project holds value in several key areas:\n",
    "1. **Improved Customer Satisfaction**: Delivers actionable insights to meet individual preferences more effectively.\n",
    "2. **Business Insights**: Helps businesses identify features that drive satisfaction, informing product improvements and marketing strategies.\n",
    "3. **Advancing E-commerce AI**: Demonstrates the power of predictive modeling in improving decision-making.\n",
    "\n",
    "In summary, this project not only enhances user satisfaction but also deepens our understanding of product feature and performance, bridging the gap between data-driven technology and personalized service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "In this step, we plan to import and preprocess the data for future usage. This includes handling missing values, extracting relevant features, and merging datasets to create a comprehensive dataframe for analysis. The preprocessing steps ensure that the data is clean and ready for modeling and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "product_df = pd.read_csv(\"Sephora/product_info.csv\")\n",
    "print(\"product info dataframe shape: \", product_df.shape)\n",
    "product_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df['size_oz'] = product_df['size'].str.extract(r'(\\d+\\.?\\d*)\\s*oz', expand=False).astype(float)\n",
    "print(product_df[['size', 'size_oz']].sample(5))\n",
    "\n",
    "product_df['size_ml'] = product_df['size'].str.extract(r'(\\d+\\.?\\d*)\\s*mL', expand=False)\n",
    "product_df['size_ml'] = product_df['size_ml'].astype(float)\n",
    "print(product_df[['size', 'size_ml']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df['price_per_oz'] = product_df['price_usd'] / product_df['size_oz']\n",
    "product_df['price_per_ml'] = product_df['price_usd'] / product_df['size_ml']\n",
    "product_df[['price_usd', 'size_oz', 'price_per_oz', 'size_ml', 'price_per_ml']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "\n",
    "# 2. Handle price-related columns by filling with price_usd\n",
    "product_df = product_df.assign(\n",
    "    child_max_price=product_df['child_max_price'].fillna(product_df['price_usd']),\n",
    "    child_min_price=product_df['child_min_price'].fillna(product_df['price_usd']),\n",
    "    sale_price_usd=product_df['sale_price_usd'].fillna(product_df['price_usd']),\n",
    "    value_price_usd=product_df['value_price_usd'].fillna(product_df['price_usd'])\n",
    ")\n",
    "\n",
    "# 3. Handle variation-related columns\n",
    "product_df = product_df.assign(\n",
    "    variation_desc=product_df['variation_desc'].fillna('No variation'),\n",
    "    variation_type=product_df['variation_type'].fillna('Unknown'),\n",
    "    variation_value=product_df['variation_value'].fillna('Unknown')\n",
    ")\n",
    "\n",
    "# 4. Handle product and review metadata\n",
    "product_df = product_df.assign(\n",
    "    tertiary_category=product_df['tertiary_category'].fillna('Uncategorized'),\n",
    "    secondary_category=product_df['secondary_category'].fillna('Uncategorized'),\n",
    ")\n",
    "\n",
    "# 6. Handle ingredient and highlight columns\n",
    "product_df = product_df.assign(\n",
    "    ingredients=product_df['ingredients'].fillna('Ingredients not listed'),\n",
    "    highlights=product_df['highlights'].fillna('No highlights available')\n",
    ")\n",
    "\n",
    "# 7. Handle size-related columns\n",
    "product_df = product_df.assign(\n",
    "    size=product_df['size'].fillna('Unknown size'),\n",
    "    size_ml=product_df['size_ml'].fillna(product_df['size_ml'].median()),\n",
    "    size_oz=product_df['size_oz'].fillna(product_df['size_oz'].median())\n",
    ")\n",
    "\n",
    "# 8. Handle price per unit columns\n",
    "product_df = product_df.assign(\n",
    "    price_per_ml=product_df['price_per_ml'].fillna(product_df['price_per_ml'].median()),\n",
    "    price_per_oz=product_df['price_per_oz'].fillna(product_df['price_per_oz'].median())\n",
    ")\n",
    "\n",
    "# 9. Print the result to confirm the missing values have been handled\n",
    "missing_counts_after = product_df.isna().sum()\n",
    "print(\"Missing values after processing:\")\n",
    "print(missing_counts_after[missing_counts_after > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows without review_rating information for analysis convenience\n",
    "product_df = product_df.dropna(subset=['rating'])\n",
    "\n",
    "# Print the result to confirm the missing values have been handled\n",
    "missing_counts_after = product_df.isna().sum()\n",
    "print(\"Missing values after processing:\")\n",
    "print(missing_counts_after[missing_counts_after > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns2 = ['size', 'size_ml']\n",
    "product_df = product_df.drop(columns=drop_columns2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Sephora: \", product_df.shape)\n",
    "product_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will set the target category for our analysis. The target category is the variable we aim to predict or analyze. For this project, we will use the `rating_category` column as our target category. This column represents the product ratings given by customers and will be used to understand and predict customer preferences and product performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df['rating_category'] = product_df['rating'].astype(float) > 4.0\n",
    "product_df.drop(columns=['rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df['rating_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['rating']\n",
    "id_cols = ['product_id', 'brand_id']\n",
    "numerical_features = ['price_usd', 'child_count', 'child_max_price', 'child_min_price', \n",
    "                    'loves_count','price_per_ml', 'price_per_oz', \n",
    "                    'reviews', 'sale_price_usd', 'size_oz', 'value_price_usd']\n",
    "categorical_features = ['new', 'online_only', 'out_of_stock', 'sephora_exclusive',\n",
    "                        'limited_edition']\n",
    "text_cols = ['product_name', 'variation_desc', 'variation_type', 'variation_value',\n",
    "             'highlights', 'secondary_category', 'tertiary_category', 'ingredients',\n",
    "             'brand_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generation by ChatGPT\n",
    "\n",
    "# Update matplotlib parameters for tighter layouts\n",
    "plt.rcParams.update({\n",
    "    'figure.autolayout': True,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'figure.constrained_layout.use': True\n",
    "})\n",
    "\n",
    "\n",
    "def plot_categorical_distribution_by_target(df, column, target=\"Transported\", palette=\"Set2\"):\n",
    "    \"\"\"\n",
    "    Plot the distribution of a categorical variable, grouped by the target variable in a 1-row, 2-column layout.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset.\n",
    "    column (str): The categorical column to visualize.\n",
    "    target (str): The target variable to group by.\n",
    "    palette (str): The color palette to use.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Extract unique categories and sort them by their first letter\n",
    "    categories = df[column].dropna().unique()\n",
    "    n = len(categories)\n",
    "    sorted_categories = sorted(categories, key=lambda x: str(x)[0])\n",
    "    \n",
    "    target_values = df[target].unique()\n",
    "    if isinstance(target_values[0], np.float64):\n",
    "        target_values = sorted(target_values)\n",
    "    colors = sns.color_palette(palette, n_colors=len(target_values))\n",
    "    fig, axes = plt.subplots(1, n, figsize=(3 * n + 2, 4), sharey=False)  # n columns per row\n",
    "    \n",
    "    for i, value in enumerate(target_values):\n",
    "        sns.countplot(\n",
    "            data=df[df[target] == value],\n",
    "            x=column,\n",
    "            order=sorted_categories,  # Pass the sorted order here\n",
    "            hue=None,\n",
    "            color=colors[i],\n",
    "            ax=axes[i]\n",
    "        )\n",
    "        axes[i].set_title(f\"{target} = {value}\")\n",
    "        axes[i].set_xlabel(column)\n",
    "        axes[i].set_ylabel(\"Count\" if i == 0 else \"\")\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle(f\"Distribution of {column} by {target}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_numeric_distribution_by_target(df, column, target=\"Transported\", bins=20, kde=True, palette=\"Set2\"):\n",
    "    \"\"\"\n",
    "    Plot the distribution of a numeric variable, grouped by the target variable in a 1-row, 2-column layout.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset.\n",
    "    column (str): The numeric column to visualize.\n",
    "    target (str): The target variable to group by.\n",
    "    bins (int): Number of bins for the histogram.\n",
    "    kde (bool): Whether to show a KDE plot.\n",
    "    palette (str): The color palette for different target values.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    target_values = df[target].unique()\n",
    "    n = len(target_values)\n",
    "    if isinstance(target_values[0], np.float64):\n",
    "        target_values = sorted(target_values)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(3 * n + 2, 4), sharey=False)  # n columns per row\n",
    "    colors = sns.color_palette(palette, n_colors=len(target_values))\n",
    "    for i, value in enumerate(target_values):\n",
    "        sns.histplot(\n",
    "            df[df[target] == value][column].dropna(),\n",
    "            bins=bins,\n",
    "            kde=kde,\n",
    "            ax=axes[i],\n",
    "            color=colors[i],\n",
    "            alpha=0.7\n",
    "        )\n",
    "        axes[i].set_title(f\"{target} = {value}\")\n",
    "        axes[i].set_xlabel(column)\n",
    "        axes[i].set_ylabel(\"Frequency\" if i == 0 else \"\")\n",
    "    plt.suptitle(f\"Distribution of {column} by {target}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_heatmap(df, target=\"rating_category\"):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of correlations between numeric variables, with respect to the target variable.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset.\n",
    "    target (str): The target variable.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\", \"bool\"]).drop(columns=[\"PassengerId\"], errors=\"ignore\")\n",
    "    correlation_matrix = numeric_cols.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    plt.title(\"Correlation Heatmap of Numeric Features\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_sephora_data_by_target(df, categorical_features, numerical_features, target=\"rating_category\"):\n",
    "    \"\"\"\n",
    "    Visualize the dataset by target, with categorical and numeric distributions shown side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset.\n",
    "    target (str): The target variable.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot categorical features\n",
    "    for feature in categorical_features:\n",
    "        plot_categorical_distribution_by_target(df, column=feature, target=target)\n",
    "    \n",
    "    # Plot numeric features\n",
    "    for feature in numerical_features:\n",
    "        plot_numeric_distribution_by_target(df, column=feature, target=target, bins=20, kde=True, palette=\"Set2\")\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plot_correlation_heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sephora_data_by_target(product_df, categorical_features, numerical_features, target=\"rating_category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Features - Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Log Transformation to several numeric features and visualize the changes\n",
    "# Use ChatGPT to facilitate data transformation\n",
    "\n",
    "def apply_log_transformation(df, features):\n",
    "    \"\"\"\n",
    "    Apply log transformation to specified numeric features in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame.\n",
    "    features (list): List of column names to log transform.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with log-transformed features.\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    for feature in features:\n",
    "        if feature in df_transformed.columns:\n",
    "            # Apply log1p to handle zeros\n",
    "            df_transformed[feature] = np.log1p(df_transformed[feature])\n",
    "        else:\n",
    "            print(f\"Feature '{feature}' not found in DataFrame.\")\n",
    "    return df_transformed\n",
    "\n",
    "def apply_sqrt_transformation(df, features):\n",
    "    \"\"\"\n",
    "    Apply square root transformation to specified numeric features in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame.\n",
    "    features (list): List of column names to apply square root transformation.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with square root-transformed features.\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    for feature in features:\n",
    "        if feature in df_transformed.columns:\n",
    "            # Apply sqrt transformation, ensure non-negative values\n",
    "            df_transformed[feature] = np.sqrt(df_transformed[feature].clip(lower=0))\n",
    "        else:\n",
    "            print(f\"Feature '{feature}' not found in DataFrame.\")\n",
    "    return df_transformed\n",
    "\n",
    "\n",
    "def visualize_features(df, features, bins=20):\n",
    "    \"\"\"\n",
    "    Visualize the distributions of specified features in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame.\n",
    "    features (list): List of column names to visualize.\n",
    "    bins (int): Number of bins for the histograms.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"Visualizing features...\")\n",
    "    print(features + [\"rating_category\"])\n",
    "    visualize_sephora_data_by_target(df[features + [\"rating_category\"]], categorical_features=[], \\\n",
    "        numerical_features=features, target=\"rating_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation\n",
    "\n",
    "features_to_transform = ['child_max_price', 'child_min_price', \n",
    "                    'reviews', 'loves_count', 'price_usd', 'price_per_ml',\n",
    "                    'price_per_oz', 'sale_price_usd', 'value_price_usd']\n",
    "\n",
    "product_df = apply_log_transformation(product_df, features_to_transform)\n",
    "\n",
    "# Visualize transformed features\n",
    "print(\"Log Transformed Features:\")\n",
    "visualize_features(product_df, features_to_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Features - TF-IDF Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "\n",
    "def tfidf_svd_transform(df, text_cols, n_components=50, max_features=500):\n",
    "    \"\"\"\n",
    "    Apply TF-IDF vectorization and SVD, and append the reduced features to the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with text columns.\n",
    "    text_cols : list\n",
    "        List of column names to process.\n",
    "    n_components : int\n",
    "        Number of components for SVD dimensionality reduction.\n",
    "    max_features : int\n",
    "        Maximum number of features for TF-IDF vectorizer.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        DataFrame with the original data and added SVD components as new columns.\n",
    "    \"\"\"\n",
    "    # Validate text columns\n",
    "    missing_cols = [col for col in text_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"The following columns are missing in the DataFrame: {missing_cols}\")\n",
    "\n",
    "    # Combine all specified text columns into a single column\n",
    "    df['combined_text'] = df[text_cols].fillna('').astype(str).agg(' '.join, axis=1)\n",
    "    \n",
    "    # Check if combined_text is empty\n",
    "    if df['combined_text'].str.strip().eq('').all():\n",
    "        raise ValueError(\"All combined_text entries are empty. Check your input data.\")\n",
    "\n",
    "    # Apply TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "    \n",
    "    # Apply SVD for dimensionality reduction\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    reduced_matrix = svd.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Create new DataFrame for SVD features\n",
    "    svd_cols = [f'svd_feature_{i+1}' for i in range(n_components)]\n",
    "    svd_df = pd.DataFrame(reduced_matrix, columns=svd_cols, index=df.index)\n",
    "    \n",
    "    # Add SVD features to the original DataFrame\n",
    "    df = pd.concat([df, svd_df], axis=1)\n",
    "    \n",
    "    # Drop the combined text column to clean up\n",
    "    df.drop(columns=['combined_text'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text columns and validate the DataFrame\n",
    "text_cols = ['product_name', 'variation_desc', 'variation_type', 'variation_value',\n",
    "             'highlights', 'secondary_category', 'tertiary_category', 'ingredients',\n",
    "             'brand_name']\n",
    "\n",
    "# Ensure product_df exists and test the function\n",
    "try:\n",
    "    product_df = tfidf_svd_transform(product_df, text_cols, n_components=50, max_features=500)\n",
    "    print(product_df.shape)\n",
    "    display(product_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, tune, and ensemble machine learning models\n",
    "\n",
    "For this part, we build multiple machine learning models, optimize their performance through hyperparameter tuning, and explore ensembling techniques to improve overall prediction accuracy.\n",
    "\n",
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "# Code generation by ChatGPT\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "product_df = product_df.drop(columns=text_cols)\n",
    "\n",
    "features = product_df.drop(columns=['rating_category', 'product_id', 'primary_category'])  # Feature data\n",
    "target = product_df['rating_category']  # Target data\n",
    "\n",
    "# Use train_test_split to split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    target, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=target\n",
    ")\n",
    "\n",
    "# Print the shapes of the split datasets\n",
    "print(\"Shape of training feature set:\", X_train.shape)\n",
    "print(\"Shape of test feature set:\", X_test.shape)\n",
    "print(\"Shape of training target set:\", y_train.shape)\n",
    "print(\"Shape of test target set:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "y_train = y_train.astype(int)  # Convert target to integer (binary classification)\n",
    "y_test = y_test.astype(int)  # Convert target to integer (binary classification)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "categorical_features = ['new', 'online_only', 'out_of_stock', 'sephora_exclusive',\n",
    "                        'limited_edition']\n",
    "num_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "# Ensure all specified features exist in X_train\n",
    "missing_categorical = [col for col in categorical_features if col not in X_train.columns]\n",
    "if missing_categorical:\n",
    "    raise ValueError(f\"Categorical features missing in X_train: {missing_categorical}\")\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", StandardScaler(), num_features),  # Apply scaling to numerical features\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the preprocessing pipeline to X_train and X_test\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Debug outputs to confirm transformation\n",
    "print(f\"Transformed X_train shape: {X_train_transformed.shape}\")\n",
    "print(f\"Transformed X_test shape: {X_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical features\n",
    "categorical_features = ['new', 'online_only', 'out_of_stock', 'sephora_exclusive',\n",
    "                        'limited_edition']\n",
    "num_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "# Ensure all specified features exist in X_train\n",
    "missing_categorical = [col for col in categorical_features if col not in X_train.columns]\n",
    "if missing_categorical:\n",
    "    raise ValueError(f\"Categorical features missing in X_train: {missing_categorical}\")\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", StandardScaler(), num_features),  # Apply scaling to numerical features\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the preprocessing pipeline to X_train and X_test\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Debug outputs to confirm transformation\n",
    "print(f\"Transformed X_train shape: {X_train_transformed.shape}\")\n",
    "print(f\"Transformed X_test shape: {X_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of model pipelines, ensemble model\n",
    "\n",
    "After data preprocessing, we define pipelines on 5 different machine learning models for comparison and potential further use. The models under evaluation are:\n",
    "- support vector machine\n",
    "- random forest\n",
    "- gradient boosting\n",
    "- xgboost\n",
    "- knn\n",
    "\n",
    "For next step, we apply a `voting classifier` technique to build an ensemble model, combining on the seperate models above as for further prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Model Pipelines\n",
    "# Define individual models\n",
    "\n",
    "# svm model\n",
    "svm_model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", SVC(\n",
    "                        probability=True, \n",
    "                        kernel=\"rbf\",  # Radial Basis Function kernel\n",
    "                        C=5,         # Regularization parameter\n",
    "                        gamma=\"auto\", # Kernel coefficient\n",
    "                        random_state=42\n",
    "                    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "random_forest_model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(\n",
    "            n_estimators=200,       # Number of trees\n",
    "            max_depth=None,           # Maximum tree depth\n",
    "            min_samples_split=4,    # Minimum number of samples required to split\n",
    "            min_samples_leaf=2,     # Minimum number of samples in a leaf node\n",
    "            max_features=\"sqrt\",    # Number of features to consider at each split\n",
    "            bootstrap=True,         # Use bootstrap samples\n",
    "            random_state=42\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting\n",
    "gradient_boosting_model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", GradientBoostingClassifier(\n",
    "            n_estimators=150,       # Number of boosting stages\n",
    "            learning_rate=0.1,     # Learning rate shrinks contribution of each tree\n",
    "            max_depth=3,            # Maximum depth of the individual regression estimators\n",
    "            min_samples_split=5,    # Minimum number of samples required to split\n",
    "            min_samples_leaf=2,     # Minimum number of samples in a leaf node\n",
    "            subsample=0.8,          # Fraction of samples to be used for fitting the individual base learners\n",
    "            random_state=42\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "xgboost_model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", XGBClassifier(\n",
    "        n_estimators=200,       # Number of boosting rounds\n",
    "        learning_rate=0.1,      # Shrinkage rate\n",
    "        max_depth=3,            # Maximum depth of a tree\n",
    "        subsample=0.8,          # Subsample ratio of training instances\n",
    "        gamma=0.2,                # Minimum loss reduction for a split\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn\n",
    "knn_model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", KNeighborsClassifier(\n",
    "        n_neighbors=15,         # Number of neighbors\n",
    "        weights=\"distance\",     # Weight function (uniform or distance)\n",
    "        algorithm=\"auto\"       # Algorithm for nearest neighbors search\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble model (Voting Classifier)\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"svm\", svm_model),\n",
    "        (\"rf\", random_forest_model),\n",
    "        (\"gb\", gradient_boosting_model),\n",
    "        (\"xgb\", xgboost_model),\n",
    "    ],\n",
    "    voting=\"soft\",  # Soft voting for probabilities,\n",
    "    weights=[0.2, 0.2, 0.3, 0.3]  # Weights for voting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation on seperate models\n",
    "In this part, we carried out cross-validation on the seperate included models as well as the initially ensembled voting classifier, with scoring-metric standards based on `accuracy`, `precision`, `recall`, `f1` and `roc_auc`. The results of each model are displayed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Cross-validation with multiple metrics in one pass\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"Support Vector Machine\": svm_model,\n",
    "    \"Random Forest\": random_forest_model,\n",
    "    \"Gradient Boosting\": gradient_boosting_model,\n",
    "    \"XGBoost\": xgboost_model,\n",
    "    \"Ensemble\": ensemble_model\n",
    "}\n",
    "\n",
    "# Define multiple scoring metrics\n",
    "scoring_metrics = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": make_scorer(precision_score, average=\"binary\"),\n",
    "    \"recall\": make_scorer(recall_score, average=\"binary\"),\n",
    "    \"f1\": make_scorer(f1_score, average=\"binary\"),\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "# Evaluate each model with cross_validate\n",
    "for model_name, model in models.items():\n",
    "    results = cross_validate(model, X_train, y_train, cv=kf, scoring=scoring_metrics)\n",
    "    cv_results[model_name] = {metric: np.mean(results[f'test_{metric}']) for metric in scoring_metrics.keys()}\n",
    "\n",
    "# Display cross-validation results\n",
    "print(\"Cross-Validation Results (by Model):\")\n",
    "for model_name, metrics in cv_results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"  {metric_name}: {score:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fit the ensemble model and predict\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "predictions = ensemble_model.predict(X_test)\n",
    "predicted_probabilities = ensemble_model.predict_proba(X_test)  # For metrics requiring probabilities\n",
    "\n",
    "# Step 5: Evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "roc_auc = roc_auc_score(y_test, predicted_probabilities[:, 1])  # Use the positive class probabilities\n",
    "logloss = log_loss(y_test, predicted_probabilities)\n",
    "\n",
    "# Display the evaluation results\n",
    "print(\"Ensemble Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generation by ChatGPT\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    \"Support Vector Machine\": {\n",
    "        \"classifier__C\": [0.5, 1, 4, 5, 6, 8, 10],  # Regularization parameter\n",
    "        \"classifier__gamma\": [\"scale\", \"auto\"],  # Kernel coefficient\n",
    "        \"classifier__kernel\": [\"rbf\", \"linear\"]  # Kernel type\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"classifier__n_estimators\": [50, 100, 150, 200],\n",
    "        \"classifier__max_depth\": [10, 15, 20, None],\n",
    "        \"classifier__min_samples_split\": [2, 5, 10],\n",
    "        \"classifier__min_samples_leaf\": [1, 2, 4]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"classifier__n_estimators\": [100, 150, 200],\n",
    "        \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.3],\n",
    "        \"classifier__max_depth\": [3, 5, 8, 10]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"classifier__n_estimators\": [100, 150, 200],\n",
    "        \"classifier__learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"classifier__max_depth\": [3, 5, 8, 10], \n",
    "        \"classifier__gamma\": [0, 0.1, 0.2, 0.5]\n",
    "    },\n",
    "    \"Ensemble\": {\n",
    "        \"weights\": [[0.2, 0.2, 0.3, 0.3], [0.25, 0.25, 0.25, 0.25], \\\n",
    "            [0.2, 0.3, 0.2, 0.3]]# Weights for voting\n",
    "        },\n",
    "}\n",
    "\n",
    "# Function to perform hyperparameter tuning\n",
    "def hyperparameter_optimization(models, param_grids, X, y):\n",
    "    optimized_models = {}\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nOptimizing {model_name}...\")\n",
    "        param_grid = param_grids.get(model_name, {})\n",
    "        search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            scoring=\"roc_auc\",  # Use AUC-ROC as the optimization metric\n",
    "            cv=5,              # 5-fold cross-validation\n",
    "            verbose=1,\n",
    "            n_jobs=-1          # Use all available cores\n",
    "        )\n",
    "        search.fit(X, y)\n",
    "        optimized_models[model_name] = search.best_estimator_\n",
    "        print(f\"Best parameters for {model_name}: {search.best_params_}\")\n",
    "        print(f\"Best ROC-AUC for {model_name}: {search.best_score_:.4f}\")\n",
    "    return optimized_models\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "optimized_models = hyperparameter_optimization(models, param_grids, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
